<!DOCTYPE html>
<html lang="en">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">

  <title>TrustNLP</title>

  <!-- Bootstrap core CSS -->
  <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Custom styles for this template -->
  <link href="css/scrolling-nav.css" rel="stylesheet">

<style>
   table {border-collapse:collapse; table-layout:fixed; width:500px;}
   table td {border:solid 1px #fab; width:100px; word-wrap:break-word;}
   </style>

</head>

<body id="page-top">

  <!-- Navigation -->
  <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top" id="mainNav">
    <div class="container">
      <a class="navbar-brand js-scroll-trigger" href="#page-top">TrustNLP @ ACL 2023</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarResponsive">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#about">About</a>
          </li>
	   <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#schedule">Schedule</a>
          </li>
	 <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#talk">Invited Talk</a>
          </li>
	 <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#papers">Papers</a>
          </li>
	
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#call_for_papers">Call for papers</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#people">People</a>
          </li>
         <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#contact">Contact</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <header class=" text-white" style="background-image: url('index.jpg')">
    <div class="container text-center">
      <h1>TrustNLP: Third Workshop on Trustworthy Natural Language Processing</h1>
      <p class="desc">Colocated with the <a style="color:white" href="https://2023.aclweb.org/"> Annual Conference of
          the Association for Computational Linguistics (ACL 2023)</a> </p>
      <p class="lead"></p>
    </div>
  </header>

  <section id="about">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto">
          <h2>About </h2>
          <p class="lead">Recent advances in Natural Language Processing, and the emergence of pretrained Large Language Models (LLM) specifically, have made NLP systems omnipresent in various aspects of our everyday life. In addition to traditional examples such as personal voice assistants, recommender systems, etc, more recent developments include content-generation models such as ChatGPT, text-to-image models (Dall-E), and so on. While these emergent technologies have an unquestionable potential to power various innovative NLP and AI applications, they also pose a number of challenges in terms of their safe and ethical use. To address such challenges, NLP researchers have formulated various objectives, e.g., intended to make models more fair, safe, and privacy-preserving. However, these objectives are often considered separately, which is a major limitation since it is often important to understand the interplay and/or tension between them. For instance, meeting a fairness objective might require access to users’ demographic information, which creates tension with privacy objectives. The goal of this workshop is to move toward a more comprehensive notion of Trustworthy NLP, by bringing together researchers working on those distinct yet related topics, as well as their intersection.
        </div>
      </div>
    </div>
  </section>


    <section id="schedule" class="bg-light">
	    
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto">
          <h2>Schedule</h2>
		<div >
<table>
<thead>
  <tr>
    <th>Name</th>
    <th>Time</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>Opening remarks </td>
    <td>9:00-9:10</td>
  </tr>
  <tr>
    <td>Keynote: Hal Daumé III, <br> Trust in the Absence of Verifiability </td>
    <td>9:10-9:50</td>
  </tr>
  <tr>
    <td>Keynote: Ramprasaath R. Selvaraju, <br>Empowering Human-like Decision Making in AI Models through Explanations </td>
    <td>9:50 -10:30</td>
  </tr>
  <tr>
    <td>Coffee Break</td>
    <td>10:30 - 11:00</td>
  </tr>
	
    <tr>
    <td>Keynote: Rachel Rudinger,<br> Trustworthy Commonsense Inference for NLP</td>
    <td>11:00-11:40</td>
  </tr>
      <tr>
    <td>Poster Session</td>
    <td>11:40-12:30</td>
  </tr>
 <tr>
    <td>Lunch</td>
    <td>12:30 - 14:00</td>
  </tr>
	
 <tr>
    <td>Keynote: Sunipa Dev,<br> Contextualizing Evaluations with Cross Cultural Perspectives</td>
    <td>14:00 - 14:40</td>
  </tr>
<tr>
    <td>Panel: Seyi Feyisetan, Madhulika Srikumar,Hal Daumé III, Sunipa Dev </td>
    <td>14:40 - 15:30</td>
  </tr>
<tr>
    <td>Coffee Break</td>
    <td>15:30 - 16:00</td>
  </tr>

<tr>
    <td>Oral presentation</td>
    <td>16:00 - 17:10</td>
  </tr>

<tr>
    <td>Award Session/Closing Remark </td>
    <td>16:00 - 17:10</td>
  </tr>
</tbody>
</table>
</div>

           </div>
      </div>
    </div>
  </section>
  <section id="talk" class="bg-light">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto">
		 <h2>Invited Talk</h2>
<h3> Hal Daumé III: Trust in the Absence of Verifiability </h3>
		<p>		When NLP systems, in particular large language models, generate claims that are easily verified by people to be untrue, trust is irrelevant. What matters is when they generate claims that are not (easily) verified to be true. Which raises two questions: (1) How much complementarity is there between what LLMs "know" and what people do, and (2) Can LLMs themselves provide missing complementary information. I'll discuss some good news and bad news that provide partial answers to these questions.
		</p>
		<p>
		Hal Daumé III is a Volpi-Cupal Endowed Professor in Computer Science and Language Science at the University of Maryland, College Park; he has a joint appointment as a Senior Principal Researcher at Microsoft Research, New York City. His primary research interest is in developing new learning algorithms for prototypical problems that arise in the context of natural language processing and artificial intelligence, with a focus on interactive learning and understanding and minimizing social harms that can be caused or exacerbated by computational systems. He has received several awards, including best paper at AACL 2022, WMT 2020, ACL 2018, NAACL 2016, CEAS 2011 and ECML 2009, as well as best demo at NeurIPS 2015. He has been program chair for ICML 2020 (together with Aarti Singh) and for NAACL 2013 (together with Katrin Kirchhoff), and he was an inaugural diversity and inclusion co-chair at NeurIPS 2018 (with Katherine Heller).
		</p>
<h3> Ramprasaath R. Selvaraju: Empowering Human-like Decision Making in AI Models through Explanations </h3>
<p>		Deep neural networks have achieved remarkable results in several Computer Vision tasks, but their increasing complexity poses challenges in terms of interpretability. In this talk, I will present my research on explainability in deep learning models, ranging from convolutional neural networks (CNNs) to multi-modal transformers, for tasks ranging from static image analysis to active perception, and demonstrate how it can enhance their human-likeness. I will focus on how interpretability can establish user trust, identify failure modes, provide targeted human feedback, debias models, ground representations, and facilitate compositional reasoning. Lastly, I will discuss future research directions. Ultimately, my talk aims to underscore the importance of interpretability in AI and its potential to advance the development of trustworthy, robust, and human-like machine learning models.
</p>
<p> Ramprasaath is a Scientist at Apple TDG (Technology Development Group). Prior to this, he was a Sr. Research Scientist at Salesforce. He holds a PhD in Computer Science from the Georgia Institute of Technology, where he was advised by Devi Parikh and Dhruv Batra. His research lies at the intersection of computer vision, explainable AI and multi-modal pretraining. Specifically, his research focuses on building algorithms that provide explanations for decisions emanating from deep networks in order to build user trust, incorporate domain knowledge into AI, and correct for unwanted biases learned by deep AI models. Previously, he has held visiting positions at Brown, Oxford, Virginia Tech, Facebook, Samsung, Tesla and Microsoft. He obtained his Bachelor's degree in Electrical and Electronics Engineering and his Master's degree in Physics from Birla Institute of Technology and Science, Pilani.</p>	
	
<h3> 		Rachel Rudinger: Trustworthy Commonsense Inference for NLP </h3>
<p>
		We use language to communicate about real-world situations, so it follows that trustworthy NLP systems ought to have basic knowledge of and ability to reason about real-world situations. Imbuing NLP systems with “common sense” requires models to learn useful generalizations about the world while avoiding the pitfalls of overgeneralization. In this talk, I will discuss a number of cases where models overgeneralize in ways that harm robustness and fairness. I will discuss work on uncovering these failure modes, as well as identifying their root causes.
</p>
		<p>Rachel Rudinger is an Assistant Professor in the Department of Computer Science at the University of Maryland, College Park. She holds joint appointments in the Department of Linguistics and the Institute for Advanced Computer Studies (UMIACS). In 2019, Rachel completed her Ph.D. in Computer Science at Johns Hopkins University in the Center for Language and Speech Processing. From 2019-2020, she was a Young Investigator at the Allen Institute for AI in Seattle, and a visiting researcher at the University of Washington. Her research interests include computational semantics, common-sense reasoning, and issues of social bias and fairness in NLP. </p>
	
	<h3> Sunipa Dev: Contextualizing Evaluations with Cross Cultural Perspectives </h3>
	<p>
		Evaluation of language models rely heavily on semi-structured data annotated by humans. Both the data and human perspectives involved in the process, thus play a key role in what is taken as ground truth by models. Historically, this perspective has been Western-oriented which leads to a lack of representation of global contexts and identities in models as well as evaluation strategies, and the risk of disregarding marginalized groups that are most significantly affected by implicit harms. Accounting for cross-cultural differences in interacting with technology is an important step for building and evaluating AI holistically.  We will talk through different strategies including community engaged approaches, along with survey experiments to capture a more diverse set of perspectives in data curation and benchmarking efforts. We will also zoom in on how socio-culturally aware AI research can fill in the gaps in fairness evaluations.
	</p>
	<p>
		Sunipa Dev is a Research Scientist at Google at their Responsible AI and Human Centered Technologies organization. She works at the intersection of language and society, trying to push the frontiers of inclusivity, equity, and representation in language technologies, and making them more socio-culturally aware. Some of her current work investigates what representation means across languages and cultures, and how to make emerging technologies more useful across the globe. Her work has received awards such as the NSF Computing Innovation Fellowship, and she was named as one of '100 Brilliant Women in AI Ethics' in 2022. She is also passionate about inclusivity in opportunities in NLP and is a core organizer of Widening NLP.
	</p>
	</div>
      </div>
    </div>
  </section>

  <section id="papers" class="bg-light">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto">
		 <h2>Accepted Papers</h2>

<li><a href="papers/2.pdf">Towards Faithful Explanations for Text Classification with Robustness Improvement and Explanation Guided Training</a>, Dongfang Li, Baotian Hu, Qingcai Chen and Shan He </li>
<li><a href="papers/3.pdf">Driving Context into Text-to-Text Privatization</a>, Stefan Arnold, Dilara Yesilbas and Sven Weinzierl </li>
<li><a href="papers/5.pdf">Automated Ableism: An Exploration of Explicit Disability Biases in Sentiment and Toxicity Analysis Models</a>, Pranav Narayanan Venkit, Mukund Srinath and Shomir Wilson </li>
<li><a href="papers/6.pdf">Pay Attention to the Robustness of Chinese Minority Language Models! Syllable-level Textual Adversarial Attack on Tibetan Script</a>, Xi Cao, Dolma Dawa, Nuo Qun and Trashi Nyima </li>
<li><a href="papers/8.pdf">Can we trust the evaluation on ChatGPT?</a>, Rachith Aiyappa, Jisun An, Haewoon Kwak and Yong-Yeol Ahn </li>
<li><a href="papers/10.pdf">Improving Factuality of Abstractive Summarization via Contrastive Reward Learning</a>, I-Chun Chern, Zhiruo Wang, Sanjan Das, Bhavuk Sharma, Pengfei Liu and Graham Neubig </li>
<li><a href="papers/11.pdf">Examining the Causal Impact of First Names on Language Models: The Case of Social Commonsense Reasoning</a>, Sullam Jeoung, Jana Diesner and Halil Kilicoglu </li>
<li><a href="papers/12.pdf">Reliability Check: An Analysis of GPT-3's Response to Sensitive Topics and Prompt Wording</a>, Aisha Khatun and Daniel G. Brown </li>
<li><a href="papers/13.pdf">On the Privacy Risk of In-context Learning</a>, Haonan Duan, Adam Dziedzic, Mohammad Yaghini, Nicolas Papernot and Franziska Boenisch </li>
<li><a href="papers/16.pdf">Sample Attackability in Natural Language Adversarial Attacks</a>, Vyas Raina and Mark Gales </li>
<li><a href="papers/17.pdf">A Keyword Based Approach to Understanding the Overpenalization of Marginalized Groups by English Marginal Abuse Models on Twitter</a>, Kyra Yee, Alice Schoenauer Sebag, Olivia Redfield, Matthias Eck, Emily Sheng and Luca Belli </li>
<li><a href="papers/18.pdf">An Empirical Study of Metrics to Measure Representational Harms in Pre-Trained Language Models</a>, Saghar Hosseini, Hamid Palangi and Ahmed Hassan Awadallah </li>
<li><a href="papers/20.pdf">Linguistic Properties of Truthful Response</a>, Bruce W. Lee, Benedict Florance Arockiaraj and Helen Jin </li>
<li><a href="papers/21.pdf">Debunking Biases in Attention</a>, Shijing Chen, Usman Naseem and Imran Razzak </li>
<li><a href="papers/22.pdf">Guiding Text-to-Text Privatization by Syntax</a>, Stefan Arnold and Dilara Yesilbas </li>
<li><a href="papers/23.pdf">Differentially Private In-Context learning</a>, Ashwinee Panda, Tong Wu, Jiachen Wang and Prateek Mittal </li>
<li><a href="papers/25.pdf">Are fairness metric scores enough to assess discrimination biases in machine learning?</a>, Fanny Jourdan, Laurent Risser, Jean-Michel Loubes and Nicholas Asher </li>
<li><a href="papers/26.pdf">DEPTH+: An Enhanced Depth Metric for Wikipedia Corpora Quality</a>, Saied Alshahrani, Norah Alshahrani and Jeanna Matthews </li>
<li><a href="papers/27.pdf">Distinguishing Fact from Fiction: A Benchmark Dataset for Identifying Machine-Generated Scientific Papers in the LLM Era.</a>, Edoardo Mosca, Mohamed Hesham Ibrahim Abdalla, Paolo Basso, Margherita Musumeci and Georg Groh </li>
<li><a href="papers/28.pdf">Detecting Personal Information in Training Corpora: an Analysis</a>, Nishant Subramani, Sasha Luccioni, Jesse Dodge and Margaret Mitchell </li>
<li><a href="papers/29.pdf">Enhancing textual counterfactual explanation intelligibility through Counterfactual Feature Importance</a>, Milan Bhan, Jean-Noel Vittaut, Nicolas Chesneau and Marie-Jeanne Lesot </li>
<li><a href="papers/32.pdf">Privacy- And Utility-Preserving NLP with Anonymized data: A case study of Pseudonymization</a>, Oleksandr Yermilov, Vipul Raheja and Artem Chernodub </li>
<li><a href="papers/35.pdf">Can NLP Models Identify, Distinguish, and Justify Questions that Don't have a Definitive Answer?</a>, Ayushi Agarwal, Nisarg Patel, Neeraj Varshney, Mihir Parmar, Pavan Mallina, Aryan Shah, Srihari Raju Sangaraju, Tirth Patel, Nihar Thakkar and Chitta Baral </li>
<li><a href="papers/36.pdf">SMoA: Sparse Mixture of Adapters to Mitigate Multiple Dataset Biases</a>, Yanchen Liu, Jing Yan, Yan Chen, Jing Liu and Hua Wu </li>
<li><a href="papers/37.pdf">GPTs Don't Keep Secrets: Searching for Backdoor Watermark Triggers in Autoregressive Language Models</a>, Evan Lucas and Timothy Havens </li>
<li><a href="papers/38.pdf">Make Text Unlearnable: Exploiting Effective Patterns to Protect Personal Data</a>, Xinzhe Li and Ming Liu </li>
<li><a href="papers/41.pdf">Bias Beyond English: Counterfactual Tests for Bias in Sentiment Analysis in Four Languages</a>, Seraphina Goldfarb-Tarrant, Adam Lopez, Roi Blanco and Diego Marcheggiani </li>
<li><a href="papers/42.pdf">Training Data Extraction From Pre-trained Language Models: A Survey</a>, Shotaro Ishihara </li>
<li><a href="papers/43.pdf">Expanding Scope: Adapting English Adversarial Attacks to Chinese</a>, Hanyu Liu, Chengyuan Cai and Yanjun Qi </li>
<li><a href="papers/45.pdf">IMBERT: Making BERT Immune to Insertion-based Backdoor Attacks</a>, Xuanli He, Jun Wang, Benjamin Rubinstein and Trevor Cohn </li>
<li><a href="papers/46.pdf">Large Language Models with Controllable Working Memory</a>, Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu and Sanjiv Kumar </li>
<li><a href="papers/47.pdf">On The Real-world Performance of Machine Translation: Exploring Social Media Post-authors' Perspectives</a>, Ananya Gupta, Jae D. Takeuchi and Bart Knijnenburg </li>
<li><a href="papers/48.pdf">ActiveAED: A Human in the Loop Improves Annotation Error Detection</a>, Leon Weber and Barbara Plank </li>
<li><a href="papers/49.pdf">Shielded Representations: Protecting Sensitive Attributes Through Iterative Gradient-Based Projection</a>, Shadi Iskander, Kira Radinsky and Yonatan Belinkov </li>
<li><a href="papers/50.pdf">Keeping Up with the Language Models: Robustness-Bias Interplay in NLI Data and Models</a>, Ioana Baldini, Chhavi Yadav, Payel Das and Kush Varshney </li>
<li><a href="papers/51.pdf">Enabling Classifiers to Make Judgements Explicitly Aligned with Human Values</a>, Yejin Bang, Tiezheng Yu, Andrea Madotto, Zhaojiang Lin, Mona Diab and Pascale Fung </li>
<li><a href="papers/52.pdf">This Prompt is Measuring <MASK>: Evaluating Bias Evaluation in Language Models</a>, Seraphina Goldfarb-Tarrant, Eddie Ungless, Esma Balkir and Su Lin Blodgett </li>
<li><a href="papers/53.pdf">COCKATIEL: COntinuous Concept ranKed ATtribution with Interpretable ELements for explaining neural net classifiers on NLP tasks</a>, Fanny Jourdan, Agustin Picard, Laurent Risser, Jean-Michel Loubes and Nicholas Asher </li>
<li><a href="papers/54.pdf">Strength in Numbers: Estimating Confidence of Large Language Models by Prompt Agreement</a>, Gwenyth Portillo Wightman, Alexandra DeLucia and Mark Dredze </li>
<li><a href="papers/55.pdf">Adversarial Named-Entity Recognition with Word Attributions and Disentanglement</a>, Xiaomeng Jin, Bhanukiran Vinzamuri, Sriram Venkatapathy, Heng Ji and Pradeep Natarajan </li>
<li><a href="papers/56.pdf">Characterizing Political Bias in Automatic Summaries: A Case Study of Trump and Biden</a>, Karen Zhou and Chenhao Tan </li>
<li><a href="papers/57.pdf">Model-tuning Via Prompts Makes NLP Models Adversarially Robust</a>, Mrigank Raman, Pratyush Maini, Zico Kolter, Zachary C. Lipton and Danish Pruthi </li>
<li><a href="papers/59.pdf">Mitigating Bias for Question Answering Models by Tracking Bias Influence</a>, Mingyu Derek Ma, Jiun-Yu Kao, Arpit Gupta, Yu-Hsiang Lin, Wenbo Zhao, Tagyoung Chung, Kai-Wei Chang and Nanyun Peng </li>
<li><a href="papers/60.pdf">PromptAttack: Probing Dialogue State Trackers with Adversarial Prompts</a>, Xiangjue Dong, Yun He, Ziwei Zhu and James Caverlee </li>
<li><a href="papers/61.pdf">Defending against Insertion-based Textual Backdoor Attacks via Attribution</a>, Jiazhao Li, Zhuofeng Wu, Wei Ping, Chaowei Xiao, V.G. Vinod Vydiswaran </li>
<li><a href="papers/62.pdf">FORK: A Bite-Sized Test Set for Probing Culinary Cultural Biases in Commonsense Reasoning Models</a>, Shramay Palta and Rachel Rudinger </li>

	
</div>
</div>
</div>
</section>
	
    <section id="call_for_papers" class="bg-light">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto">
          <h2>Call for papers</h2>
          <p class="lead" {color: blue;}><b>Overview</b></p>
         <p dir="ltr"><span style="font-size:12pt;font-family:Arial;color:#212529;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">We invite papers which focus on developing models that are &ldquo;explainable, fair, privacy-preserving, causal, and robust&rdquo; (Trustworthy ML Initiative). Topics of interest include (but are not limited to):</span></p>
<ul>
    <li dir="ltr" ><span style="font-size:12pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Differential Privacy</span></li>
    <li dir="ltr" ><span style="font-size:12pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Fairness and Bias: Evaluation and Treatments</span></li>
    <li dir="ltr" ><span style="font-size:12pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Model Explainability and Interpretability</span></li>
    <li dir="ltr" ><span style="font-size:12pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Accountability, Safety, and Robustness</span></li>
     <li dir="ltr" ><span
             style="font-size:12pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Ethics</span></li>
<li dir="ltr" ><span style="font-size:12pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Industry applications of Trustworthy NLP</span></li>
    <li dir="ltr" ><span style="font-size:12pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Causal Inference and Fair ML</span></li>
    <li dir="ltr" ><span style="font-size:12pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Secure, Faithful, Trustworthy Data/Language Generation</span></li>
<li dir="ltr" ><span style="font-size:12pt;font-family:Arial;color:#000000;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Toxic Language Detection and Mitigation</span></li>

	We also welcome contributions which draw upon interdisciplinary knowledge to advance Trustworthy NLP.  This may include working with, synthesizing, or incorporating knowledge across expertise, sociopolitical systems, cultures, or norms.
	
	
</ul>
<p class="lead"><b>Important Dates</b></p>
<ul>
    <li>
        <h3 dir="ltr" ><span
                style="font-size:12pt;font-family:Arial;color:#222222;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">Apr 24, 2023: Workshop Paper Due Date (Direct Submission)</span></h3>
    </li>
    <li>
        <h3 dir="ltr" ><span
                style="font-size:12pt;font-family:Arial;color:#222222;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">May 10, 2023: Workshop Paper Due Date (Fast-Track)</span></h3>
    </li>
    <li>
        <h3 dir="ltr"><span style="font-size:12pt;font-family:Arial;color:#222222;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">May 22, 2023: Notification of Acceptance</span></h3>
    </li>
    <li>
        <h3 dir="ltr"><span style="font-size:12pt;font-family:Arial;color:#222222;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">June 2, 2023: Camera-ready papers due</span></h3>
    </li>
    <li>
        <h3 dir="ltr" ><span style="font-size:12pt;font-family:Arial;color:#222222;background-color:transparent;font-weight:400;font-style:normal;font-variant:normal;text-decoration:none;vertical-align:baseline;white-space:pre;white-space:pre-wrap;">July 14, 2023: Workshop</span></h3>
    </li>
</ul>

          <p class="lead"><b>Submission Policy</b></p>
          <p>All submissions will be double-blind peer-reviewed (with author names and affiliations removed) by the
              program committee and judged by their relevance to the workshop themes. </br> 
	      Accepted and under-review papers are allowed to submit to the workshop but will not be included in the proceeding.

Submitted manuscripts must be 8 pages long for full papers, and 4 pages long for short papers. Please follow <a href="https://2023.aclweb.org/calls/main_conference/#paper-submission-policies"> ACL submission policies. </a> Both full and short papers can have unlimited pages for references and appendices. Please note that at least one of the authors of each accepted paper must register for the workshop and present the paper.
Template files can be found here: https://aclrollingreview.org/cfp#long-papers.</p>

<p>We also ask authors to include a limitation section and broader impact statement, following guidelines from the main
            conference.
</p>
            <p> Please submit to Softconf via this <a href="https://softconf.com/acl2023/trustnlp2023/">link</a></p>

	  <p class="lead"><b>Fast-Track Submission</b></p>
          <p> If your paper has been reviewed by ACL, EMNLP, EACL, or ARR and the average rating is higher than 2.5 (either avg soundness or excitement score), the paper is qualified to be submitted to the fast-track. 
	      In the appendix, please include the reviews and a short statement discussing what parts of the paper have been revised. 
		 
          <p class="lead"><b>Non-Archival option</b></p>
          <p>ACL workshops are traditionally archival. To allow dual submission of work, we are also including a non-archival track. If accepted, these submissions will still participate and present their work in the workshop. A reference to the paper will be hosted on the workshop website (if desired), but will not be included in the official proceedings. Please submit through softconf but indicate that this is a cross submission at the bottom of the submission form. You can also skip this step and inform us of your non-archival preference after the reviews.</p>

          <p class="lead"><b>Anonymity Period</b></p>
          <p>We will follow NAACL’s anonymity policy, and require full anonymity until time of acceptance. </p>

        </div>
      </div>
    </div>
  </section>

  <section id="people">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto">
          <h2>Committee</h2>
          <p class="lead">Organizers</p>
	<ul>
	  <li>	  <a href="http://yadapruk.com"> Yada Pruksachatkun </a> - Infinitus Systems </li>
		<li>	  <a href="http://scf.usc.edu/~ninarehm/"> Ninareh Mehrabi</a> - Amazon Alexa AI </li>
	  <li>    <a href="http://web.cs.ucla.edu/~kwchang/"> Kai-Wei Chang </a> - UCLA, Amazon Visiting Academic</li>
    <li>    <a href="https://www.isi.edu/people/galstyan/about"> Aram Galystan </a> - USC, Amazon Visiting Academic</li>
 	  <li>    <a href="https://jwaladhamala.com/"> Jwala Dhamala</a> - Amazon Alexa AI</li>
		  <li>    <a href="https://anaeliaovalle.github.io/"> Anaelia Ovalle</a> - UCLA</li>
 	  <li>    <a href="https://github.com/dapurv5"> Apurv Verma</a> - Amazon Alexa AI</li>
    <li>    <a href="http://ycao95.umiacs.io/"> Yang Trista Cao</a> - University of Maryland</li>
		<li> <a href="https://www.amazon.science/author/anoop-kumar">Anoop Kumar </a> - Amazon Alexa AI </li>
		   <li>    <a href="https://guptarah.github.io/"> Rahul Gupta</a> - Amazon Alexa AI</li>
	</ul>  	
          <p class="lead">Program committee</p>
		<div>

	<ul>

<li>Griffin Adams, Columbia University </li>
<li>Stefan Arnold, FAU Erlangen-Nurnberg</li>
<li>Connor Baumler, University of Maryland</li>
<li>Keith Burghardt, USC Information Sciences Institute</li>
<li>Yang Trista Cao, University of Maryland</li>
<li>Jwala Dhamala, Amazon Alexa AI-NLU</li>
<li>Jacob Eisenstein, Google</li>
<li>Katja Filippova, Google</li>
<li>Aram Galstyan, USC Information Sciences Institute</li>
<li>Umang Gupta, University of Southern California</li>
<li>Devamanyu Hazarika, Amazon</li>
<li>Zihao He, University of Southern California</li>
<li>William Held, Georgia Tech</li>
<li>Qian Hu, Amazon.com</li>
<li>Fatemah Husain, Kuwait University</li>
<li>Anoop Kumar, Amazon</li>
<li>Sasha Luccioni, Hugging Face</li>
<li>Pranav Narayanan Venkit, Pennsylvania State University</li>
<li>Isar Nejadgholi, National Research Council Canada</li>
<li>Aishwarya Padmakumar, Amazon</li>
<li>Ashwinee Panda, Princeton University</li>
<li>Anirudh Raju, Amazon, Alexa</li>
<li>Anthony Rios, University of Texas at San Antonio</li>
<li>Robik Shrestha, RIT</li>
<li>Anna Sotnikova, University of Maryland</li>
<li>Arjun Subramonian, University of California, Los Angeles</li>
<li>Jialu Wang, University of California, Santa Cruz</li>
<li>Chhavi Yadav, UCSD</li>
<li>Kiyoon Yoo, Seoul National University</li>

	</ul>  	
		</div>

        </div>
      </div>
    </div>
  </section>


  <section id="speakers" class="bg-light">
    <div class="container" style="display: none">
      <div class="row">
        <div class="col-lg-8 mx-auto">
          <h2>Speakers</h2>
            </br>
            <img src="Diyi_Yang.jpeg" class="icons">
            <strong> <p style="text-align: center; font-size: 20px"> Diyi Yang, Assistant Professor, School of Interactive Computing, Georgia Tech </p></strong>
            <p  style="text-align: center;">Diyi Yang is an assistant professor in the School of Interactive Computing at Georgia Tech. Diyi has broad interests in NLP and Computational Social Science, including dialogue summarization, limited data learning,  hate speech and bias,  as well as responsible NLP for mental health. Her work has received multiple awards (or nominations) at EMNLP, ICWSM, SIGCHI, and CSCW. She is a Microsoft Research Faculty Fellow, a Forbes 30 under 30 in Science, an IEEE “AI 10 to Watch”, a recipient of the Intel Rising Star Faculty Award, the Samsung AI Researcher of the Year and the NSF CAREER Award.</p>

            </br>
            <img src="sbmisi.jpeg" class="icons">
            <strong> <p  style="text-align: center;">Subho Majumdar, Senior Scientist, Splunk</p> </strong>
            <p style="text-align: center;"> Subho is a senior scientist in the Applied ML Research team of Splunk. Before recently joining Splunk, he spent 3 years in AT&T Data Science and AI Research (erstwhile part of AT&T Bell Labs). Throughout his career, he has worked on data-driven solutions that pushed the boundaries for a variety of challenging and cross-product problems. His current focus is on trustworthy machine learning in the wild: not only proposing novel solutions to technical problems that ensure qualities such as fairness, transparency, privacy, and robustness, but also implementing them in real-world use cases.</p>

            </br>
            <img src="Fei_Wang.jpeg" class="icons">
            <strong> <p  style="text-align: center;">Fei Wang, Associate Professor in Division of Health Informatics, Cornell University</p> </strong>
            <p style="text-align: center;">Fei Wang is an Associate Professor in Division of Health Informatics, Department of Population Health Sciences, Weill Cornell Medicine, Cornell University. His major research interest is data mining, machine learning and their applications in health data science. He has published more than 250 papers on the top venues of related areas such as ICML, KDD, NIPS, CVPR, AAAI, IJCAI, JAMA Internal Medicine, Annals of Internal Medicine, Lancet Digital Health, etc. His papers have received over 19,000 citations so far with an H-index 67. His (or his students’) papers have won 8 best paper (or nomination) awards at top international conferences on data mining and medical informatics. His team won the championship of the NIPS/Kaggle Challenge on Classification of Clinically Actionable Genetic Mutations in 2017 and Parkinson's Progression Markers' Initiative data challenge organized by Michael J. Fox Foundation in 2016. Dr. Wang is the recipient of the NSF CAREER Award in 2018, as well as the inaugural research leadership award in IEEE International Conference on Health Informatics (ICHI) 2019. Dr. Wang’s Research has been supported by NSF, NIH, ONR, PCORI, MJFF, AHA, Amazon, etc. Dr. Wang is the past chair of the Knowledge Discovery and Data Mining working group in American Medical Informatics Association (AMIA). Dr. Wang is a fellow of AMIA and a Distinguished Member of ACM.</p>
        </div>
      </div>
    </div>
  </section>


  <section id="contact" >
    <div class="container">
      <div class="row">
        <div class="col-lg-8 mx-auto">
          <p class="lead">  For questions, please contact us at trustnlpworkshoporganizers@gmail.com </p>
        </div>
      </div>
    </div>
  </section>
  <!-- Footer -->
  <footer class="py-5 bg-dark">
    <div class="container">
      <p class="m-0 text-center text-white">Copyright &copy; TrustNLP 2023</p>
    </div>
    <!-- /.container -->
  </footer>

  <!-- Bootstrap core JavaScript -->
  <script src="vendor/jquery/jquery.min.js"></script>
  <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

  <!-- Plugin JavaScript -->
  <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

  <!-- Custom JavaScript for this theme -->
  <script src="js/scrolling-nav.js"></script>

</body>

</html>
